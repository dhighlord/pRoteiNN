{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sak1b0/proteiNN/blob/master/forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uhMU-BH1HBT",
        "colab_type": "code",
        "outputId": "29e3a549-e7b9-4920-80d3-e867e5d66207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11460
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import losses\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "prop = {1:[1.8,-0.17,0.11,0,0.38,-0.21,-1.6,0.42,-0.27,1.12,0.61],\n",
        "18:[-4.5,-0.81,2.58,3.71,-2.57,2.11,12.3,-1.56,1.87,-2.55,0.6],\n",
        "14:[-3.5,-0.42,2.05,3.47,-1.62,0.96,4.8,-1.03,0.81,-0.83,0.06],\n",
        "4:[-3.5,-1.23,3.49,2.95,-3.27,1.36,9.2,-0.51,0.81,-0.83,0.46],\n",
        "3:[2.5,0.24,-0.13,0.49,-0.3,-6.04,-2,0.84,-1.05,0.59,1.07],\n",
        "17:[-3.5,-0.58,2.36,3.01,-1.84,1.52,4.1,-0.96,1.1,-0.78,0],\n",
        "5:[-3.5,-2.02,2.68,1.64,-2.9,2.3,8.2,-0.37,1.17,-0.92,0.47],\n",
        "7:[-0.4,-0.01,0.74,1.72,-0.19,0,-1,0,-0.16,1.2,0.07],\n",
        "8:[-3.2,-0.96,2.06,4.76,-1.44,-1.23,3,-2.28,0.28,-0.93,0.61],\n",
        "9:[4.5,0.31,-0.6,-1.56,1.97,-4.81,-3.1,1.81,-0.77,1.16,2.22],\n",
        "12:[3.8,0.56,-0.55,-1.81,1.82,-4.68,-2.8,1.8,-1.1,1.18,1.53],\n",
        "11:[-3.9,-0.99,2.71,5.39,-3.46,3.88,8.8,-2.03,1.7,-0.8,1.15],\n",
        "13:[1.9,0.23,-0.1,-0.76,1.4,-3.66,-3.4,1.18,-0.73,0.55,1.18],\n",
        "6:[2.8,1.13,-0.32,-2.2,1.98,-4.65,-3.7,1.74,-1.43,0.67,2.02],\n",
        "16:[-1.6,-0.45,2.23,-1.52,-1.44,0.75,0.2,0.86,-0.75,0.54,1.95],\n",
        "19:[-0.8,-0.13,0.84,1.83,-0.53,1.74,-0.6,-0.64,0.42,-0.05,0.05],\n",
        "20:[-0.7,-0.14,0.52,1.78,-0.32,0.78,-1.2,-0.26,0.63,-0.02,0.05],\n",
        "23:[-0.9,1.85,0.3,-0.38,1.53,-3.32,-1.9,1.46,-1.57,-0.19,2.65],\n",
        "25:[-1.3,0.94,0.68,-1.09,0.49,-1.01,0.7,0.51,-0.56,-0.23,1.88],\n",
        "22:[4.2,-0.07,-0.31,-0.78,1.46,-3.5,-2.6,1.34,-0.4,1.13,1.32],\n",
        "26:[0,0,0,0,0,0,0,0,0,0,0]}\n",
        "\n",
        "df_train=np.asarray(pd.read_csv('https://raw.githubusercontent.com/sak1b0/proteiNN/master/train_formatted.csv',header=None))\n",
        "df_test=np.asarray(pd.read_csv('https://raw.githubusercontent.com/sak1b0/proteiNN/master/test_formatted.csv',header=None))\n",
        "\n",
        "x_train = df_train[:,0]\n",
        "y_train = df_train[:,1]\n",
        "\n",
        "x_test = df_test[:,0]\n",
        "y_test = df_test[:,1]\n",
        "\n",
        "def debug_me():\n",
        "  #print('train dataframe: ',df_train.shape)\n",
        "  print('x_train shape: ',x_train.shape)\n",
        "  print('y_train shape: ',y_train.shape)\n",
        "\n",
        "  #print('test dataframe: ',df_test.shape)\n",
        "  print('x_test shape: ',x_test.shape)\n",
        "  print('y_test shape: ',y_test.shape)\n",
        "\n",
        "max_len=400\n",
        "\n",
        "#================== x_train ===============\n",
        "n = x_train\n",
        "j=-1\n",
        "\n",
        "for i in x_train:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "x_train = n\n",
        "\n",
        "#================= y_train =================\n",
        "n = y_train\n",
        "j=-1\n",
        "\n",
        "for i in y_train:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "    \n",
        "y_train = n\n",
        "#=================  x_test ==================\n",
        "n = x_test\n",
        "j=-1\n",
        "\n",
        "for i in x_test:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "x_test = n\n",
        "#=================  y_test ==================\n",
        "n = y_test\n",
        "j=-1\n",
        "\n",
        "for i in y_test:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "y_test = n\n",
        "\n",
        "#============= selected data withing range===========\n",
        "\n",
        "\n",
        "max_len = max([len(i) for i in x_train])\n",
        "#print(max_len)\n",
        "\n",
        "max_len = max([len(i) for i in y_test])\n",
        "#print(max_len)\n",
        "\n",
        "print('starting the preprocessing\\n')\n",
        "start_time = time.time()\n",
        "\n",
        "#==============   Properties Encoded start  ============================\n",
        "\n",
        "# ==========x_train conversion start====\n",
        "s = list(x_train)\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(s)):\n",
        "  t=[]\n",
        "  for item in range(len(s[i])):\n",
        "    t.append(prop[ord(s[i][item])-64])\n",
        "  k.append(t)\n",
        "\n",
        "\n",
        "x_train = np.array(k)\n",
        "\n",
        "#=========== x_train conversion end ====\n",
        "\n",
        "#=========== x_test conversion start====\n",
        "s = list(x_test)\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(s)):\n",
        "  t=[]\n",
        "  for item in range(len(s[i])):\n",
        "    t.append(prop[ord(s[i][item])-64])\n",
        "  k.append(t)\n",
        "\n",
        "\n",
        "x_test = np.array(k)\n",
        "\n",
        "#============= x_test conversion end====\n",
        "\n",
        "\n",
        "#==============   Properties Encoded end  ============================\n",
        "\n",
        "\n",
        "\n",
        "#==============   ONE_HOT   ===================================================\n",
        "\n",
        "#======= y_train start========\n",
        "#y_train = y_train[0:3]\n",
        "\n",
        "alphabet = 'CEHXZ'\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  integer_encoded = [char_to_int[char] for char in y_train[i]]\n",
        "  \n",
        "  onehot_encoded=list()\n",
        "  for value in integer_encoded:\n",
        "\t  letter = [0 for _ in range(len(alphabet))]\n",
        "\t  letter[value] = 1\n",
        "\t  onehot_encoded.append(letter)\n",
        "  \n",
        "  k.append(onehot_encoded)  \n",
        "\n",
        "y_train = np.array(k)\n",
        "#display(y_train)\n",
        "\n",
        "#======= y_train end========\n",
        "\n",
        "#======= y_test start========\n",
        "#y_train = y_train[0:3]\n",
        "\n",
        "alphabet = 'CEHXZ'\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "  integer_encoded = [char_to_int[char] for char in y_test[i]]\n",
        "  \n",
        "  onehot_encoded=list()\n",
        "  for value in integer_encoded:\n",
        "\t  letter = [0 for _ in range(len(alphabet))]\n",
        "\t  letter[value] = 1\n",
        "\t  onehot_encoded.append(letter)\n",
        "  \n",
        "  k.append(onehot_encoded)  \n",
        "\n",
        "y_test = np.array(k)\n",
        "#display(y_train)\n",
        "\n",
        "#======= y_test end========\n",
        "\n",
        "#==============   ONE_HOT   finish ============================\n",
        "\n",
        "print('ending the preprocessing\\n')\n",
        "finish_time=time.time()\n",
        "print ('Time taken to pre-process: ',round(finish_time - start_time,2),' seconds')\n",
        "\n",
        "#==============   ONE_HOT_INVERSION   =========================================\n",
        " \n",
        "#for i in range(len(y_train[0])):\n",
        "#  inverted = int_to_char[argmax(y_train[0][i])]\n",
        "#  print(inverted)\n",
        "\n",
        "#================ it's time to learn============================\n",
        "debug_me()\n",
        "start_time = time.time()\n",
        "  \n",
        "model=Sequential()\n",
        "\n",
        "model.add(LSTM((5),batch_input_shape=(None,400,11),return_sequences=True,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(model.input_shape)\n",
        "print(model.output_shape)\n",
        "\n",
        "history=model.fit(x_train,y_train,epochs=500,validation_data=(x_test,y_test))\n",
        "\n",
        "finish_time=time.time()\n",
        "print ('Time taken to train: ',round((finish_time - start_time)/60,2),' minutes')\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting the preprocessing\n",
            "\n",
            "ending the preprocessing\n",
            "\n",
            "Time taken to pre-process:  7.15  seconds\n",
            "x_train shape:  (4061, 400, 11)\n",
            "y_train shape:  (4061, 400, 5)\n",
            "x_test shape:  (1058, 400, 11)\n",
            "y_test shape:  (1058, 400, 5)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 400, 5)            340       \n",
            "=================================================================\n",
            "Total params: 340\n",
            "Trainable params: 340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(None, 400, 11)\n",
            "(None, 400, 5)\n",
            "Train on 4061 samples, validate on 1058 samples\n",
            "Epoch 1/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 2.3136 - acc: 0.6537 - val_loss: 1.9413 - val_acc: 0.7151\n",
            "Epoch 2/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 1.6911 - acc: 0.7074 - val_loss: 1.4924 - val_acc: 0.7290\n",
            "Epoch 3/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 1.3318 - acc: 0.7153 - val_loss: 1.0061 - val_acc: 0.7371\n",
            "Epoch 4/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9911 - acc: 0.7212 - val_loss: 0.9112 - val_acc: 0.7395\n",
            "Epoch 5/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9578 - acc: 0.7229 - val_loss: 0.8971 - val_acc: 0.7406\n",
            "Epoch 6/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.9474 - acc: 0.7243 - val_loss: 0.8892 - val_acc: 0.7416\n",
            "Epoch 7/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9404 - acc: 0.7259 - val_loss: 0.8840 - val_acc: 0.7421\n",
            "Epoch 8/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9351 - acc: 0.7271 - val_loss: 0.8793 - val_acc: 0.7438\n",
            "Epoch 9/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9305 - acc: 0.7282 - val_loss: 0.8753 - val_acc: 0.7448\n",
            "Epoch 10/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9264 - acc: 0.7297 - val_loss: 0.8719 - val_acc: 0.7459\n",
            "Epoch 11/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9231 - acc: 0.7306 - val_loss: 0.8694 - val_acc: 0.7468\n",
            "Epoch 12/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9199 - acc: 0.7318 - val_loss: 0.8667 - val_acc: 0.7476\n",
            "Epoch 13/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9174 - acc: 0.7325 - val_loss: 0.8642 - val_acc: 0.7488\n",
            "Epoch 14/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9149 - acc: 0.7332 - val_loss: 0.8622 - val_acc: 0.7492\n",
            "Epoch 15/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9130 - acc: 0.7337 - val_loss: 0.8606 - val_acc: 0.7498\n",
            "Epoch 16/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9113 - acc: 0.7345 - val_loss: 0.8593 - val_acc: 0.7503\n",
            "Epoch 17/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9100 - acc: 0.7348 - val_loss: 0.8579 - val_acc: 0.7510\n",
            "Epoch 18/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9086 - acc: 0.7356 - val_loss: 0.8571 - val_acc: 0.7508\n",
            "Epoch 19/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9073 - acc: 0.7361 - val_loss: 0.8559 - val_acc: 0.7516\n",
            "Epoch 20/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9061 - acc: 0.7365 - val_loss: 0.8544 - val_acc: 0.7523\n",
            "Epoch 21/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9049 - acc: 0.7371 - val_loss: 0.8534 - val_acc: 0.7529\n",
            "Epoch 22/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9038 - acc: 0.7376 - val_loss: 0.8523 - val_acc: 0.7532\n",
            "Epoch 23/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.9027 - acc: 0.7380 - val_loss: 0.8514 - val_acc: 0.7537\n",
            "Epoch 24/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9016 - acc: 0.7387 - val_loss: 0.8502 - val_acc: 0.7541\n",
            "Epoch 25/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9004 - acc: 0.7392 - val_loss: 0.8492 - val_acc: 0.7545\n",
            "Epoch 26/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8492 - acc: 0.7383 - val_loss: 0.7794 - val_acc: 0.7551\n",
            "Epoch 27/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8194 - acc: 0.7405 - val_loss: 0.7752 - val_acc: 0.7564\n",
            "Epoch 28/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8127 - acc: 0.7429 - val_loss: 0.7698 - val_acc: 0.7571\n",
            "Epoch 29/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8096 - acc: 0.7431 - val_loss: 0.7676 - val_acc: 0.7582\n",
            "Epoch 30/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8084 - acc: 0.7435 - val_loss: 0.7668 - val_acc: 0.7582\n",
            "Epoch 31/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8078 - acc: 0.7437 - val_loss: 0.7667 - val_acc: 0.7575\n",
            "Epoch 32/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8071 - acc: 0.7437 - val_loss: 0.7660 - val_acc: 0.7585\n",
            "Epoch 33/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8065 - acc: 0.7442 - val_loss: 0.7652 - val_acc: 0.7589\n",
            "Epoch 34/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8056 - acc: 0.7449 - val_loss: 0.7647 - val_acc: 0.7594\n",
            "Epoch 35/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8046 - acc: 0.7456 - val_loss: 0.7641 - val_acc: 0.7594\n",
            "Epoch 36/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8037 - acc: 0.7458 - val_loss: 0.7636 - val_acc: 0.7598\n",
            "Epoch 37/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8032 - acc: 0.7458 - val_loss: 0.7633 - val_acc: 0.7592\n",
            "Epoch 38/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8028 - acc: 0.7459 - val_loss: 0.7629 - val_acc: 0.7595\n",
            "Epoch 39/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.7431 - acc: 0.7457 - val_loss: 0.5253 - val_acc: 0.7591\n",
            "Epoch 40/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5537 - acc: 0.7456 - val_loss: 0.5225 - val_acc: 0.7592\n",
            "Epoch 41/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5517 - acc: 0.7460 - val_loss: 0.5216 - val_acc: 0.7598\n",
            "Epoch 42/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5507 - acc: 0.7462 - val_loss: 0.5204 - val_acc: 0.7600\n",
            "Epoch 43/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5499 - acc: 0.7466 - val_loss: 0.5199 - val_acc: 0.7598\n",
            "Epoch 44/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5494 - acc: 0.7466 - val_loss: 0.5195 - val_acc: 0.7605\n",
            "Epoch 45/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5488 - acc: 0.7468 - val_loss: 0.5187 - val_acc: 0.7607\n",
            "Epoch 46/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5482 - acc: 0.7471 - val_loss: 0.5189 - val_acc: 0.7601\n",
            "Epoch 47/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5479 - acc: 0.7471 - val_loss: 0.5181 - val_acc: 0.7606\n",
            "Epoch 48/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5473 - acc: 0.7475 - val_loss: 0.5187 - val_acc: 0.7592\n",
            "Epoch 49/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5472 - acc: 0.7474 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 50/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5465 - acc: 0.7477 - val_loss: 0.5168 - val_acc: 0.7610\n",
            "Epoch 51/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5463 - acc: 0.7476 - val_loss: 0.5167 - val_acc: 0.7609\n",
            "Epoch 52/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5459 - acc: 0.7477 - val_loss: 0.5163 - val_acc: 0.7616\n",
            "Epoch 53/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5456 - acc: 0.7478 - val_loss: 0.5161 - val_acc: 0.7616\n",
            "Epoch 54/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5453 - acc: 0.7478 - val_loss: 0.5155 - val_acc: 0.7616\n",
            "Epoch 55/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5449 - acc: 0.7481 - val_loss: 0.5154 - val_acc: 0.7618\n",
            "Epoch 56/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5447 - acc: 0.7479 - val_loss: 0.5154 - val_acc: 0.7620\n",
            "Epoch 57/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5443 - acc: 0.7483 - val_loss: 0.5151 - val_acc: 0.7618\n",
            "Epoch 58/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5441 - acc: 0.7484 - val_loss: 0.5150 - val_acc: 0.7614\n",
            "Epoch 59/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5439 - acc: 0.7483 - val_loss: 0.5145 - val_acc: 0.7615\n",
            "Epoch 60/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5438 - acc: 0.7484 - val_loss: 0.5144 - val_acc: 0.7620\n",
            "Epoch 61/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5435 - acc: 0.7485 - val_loss: 0.5142 - val_acc: 0.7623\n",
            "Epoch 62/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5432 - acc: 0.7486 - val_loss: 0.5142 - val_acc: 0.7615\n",
            "Epoch 63/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5429 - acc: 0.7487 - val_loss: 0.5138 - val_acc: 0.7623\n",
            "Epoch 64/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5428 - acc: 0.7488 - val_loss: 0.5136 - val_acc: 0.7625\n",
            "Epoch 65/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5426 - acc: 0.7489 - val_loss: 0.5135 - val_acc: 0.7626\n",
            "Epoch 66/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5423 - acc: 0.7490 - val_loss: 0.5134 - val_acc: 0.7628\n",
            "Epoch 67/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5420 - acc: 0.7492 - val_loss: 0.5139 - val_acc: 0.7620\n",
            "Epoch 68/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5421 - acc: 0.7491 - val_loss: 0.5130 - val_acc: 0.7629\n",
            "Epoch 69/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5419 - acc: 0.7493 - val_loss: 0.5134 - val_acc: 0.7618\n",
            "Epoch 70/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5417 - acc: 0.7494 - val_loss: 0.5127 - val_acc: 0.7628\n",
            "Epoch 71/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5413 - acc: 0.7499 - val_loss: 0.5123 - val_acc: 0.7629\n",
            "Epoch 72/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5412 - acc: 0.7497 - val_loss: 0.5122 - val_acc: 0.7629\n",
            "Epoch 73/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5409 - acc: 0.7500 - val_loss: 0.5120 - val_acc: 0.7632\n",
            "Epoch 74/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5407 - acc: 0.7499 - val_loss: 0.5117 - val_acc: 0.7632\n",
            "Epoch 75/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5407 - acc: 0.7499 - val_loss: 0.5122 - val_acc: 0.7632\n",
            "Epoch 76/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5403 - acc: 0.7504 - val_loss: 0.5117 - val_acc: 0.7634\n",
            "Epoch 77/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5401 - acc: 0.7502 - val_loss: 0.5118 - val_acc: 0.7637\n",
            "Epoch 78/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5400 - acc: 0.7505 - val_loss: 0.5115 - val_acc: 0.7634\n",
            "Epoch 79/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5397 - acc: 0.7505 - val_loss: 0.5112 - val_acc: 0.7636\n",
            "Epoch 80/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5395 - acc: 0.7506 - val_loss: 0.5108 - val_acc: 0.7639\n",
            "Epoch 81/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5392 - acc: 0.7508 - val_loss: 0.5107 - val_acc: 0.7640\n",
            "Epoch 82/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5389 - acc: 0.7510 - val_loss: 0.5104 - val_acc: 0.7641\n",
            "Epoch 83/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5387 - acc: 0.7511 - val_loss: 0.5101 - val_acc: 0.7643\n",
            "Epoch 84/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5386 - acc: 0.7511 - val_loss: 0.5104 - val_acc: 0.7642\n",
            "Epoch 85/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5384 - acc: 0.7513 - val_loss: 0.5096 - val_acc: 0.7648\n",
            "Epoch 86/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5380 - acc: 0.7515 - val_loss: 0.5093 - val_acc: 0.7650\n",
            "Epoch 87/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5375 - acc: 0.7521 - val_loss: 0.5089 - val_acc: 0.7653\n",
            "Epoch 88/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5371 - acc: 0.7521 - val_loss: 0.5090 - val_acc: 0.7651\n",
            "Epoch 89/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5368 - acc: 0.7523 - val_loss: 0.5087 - val_acc: 0.7649\n",
            "Epoch 90/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5368 - acc: 0.7524 - val_loss: 0.5089 - val_acc: 0.7649\n",
            "Epoch 91/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5362 - acc: 0.7527 - val_loss: 0.5082 - val_acc: 0.7655\n",
            "Epoch 92/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5361 - acc: 0.7528 - val_loss: 0.5080 - val_acc: 0.7661\n",
            "Epoch 93/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5357 - acc: 0.7531 - val_loss: 0.5078 - val_acc: 0.7655\n",
            "Epoch 94/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5356 - acc: 0.7532 - val_loss: 0.5074 - val_acc: 0.7664\n",
            "Epoch 95/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5353 - acc: 0.7533 - val_loss: 0.5083 - val_acc: 0.7658\n",
            "Epoch 96/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5350 - acc: 0.7537 - val_loss: 0.5073 - val_acc: 0.7661\n",
            "Epoch 97/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5347 - acc: 0.7538 - val_loss: 0.5064 - val_acc: 0.7665\n",
            "Epoch 98/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5345 - acc: 0.7540 - val_loss: 0.5070 - val_acc: 0.7659\n",
            "Epoch 99/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5343 - acc: 0.7542 - val_loss: 0.5079 - val_acc: 0.7650\n",
            "Epoch 100/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5344 - acc: 0.7542 - val_loss: 0.5075 - val_acc: 0.7660\n",
            "Epoch 101/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5338 - acc: 0.7547 - val_loss: 0.5058 - val_acc: 0.7672\n",
            "Epoch 102/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5336 - acc: 0.7549 - val_loss: 0.5064 - val_acc: 0.7663\n",
            "Epoch 103/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5336 - acc: 0.7547 - val_loss: 0.5055 - val_acc: 0.7672\n",
            "Epoch 104/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5332 - acc: 0.7549 - val_loss: 0.5053 - val_acc: 0.7675\n",
            "Epoch 105/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5331 - acc: 0.7550 - val_loss: 0.5053 - val_acc: 0.7673\n",
            "Epoch 106/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5328 - acc: 0.7554 - val_loss: 0.5058 - val_acc: 0.7674\n",
            "Epoch 107/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5326 - acc: 0.7552 - val_loss: 0.5053 - val_acc: 0.7675\n",
            "Epoch 108/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5324 - acc: 0.7556 - val_loss: 0.5046 - val_acc: 0.7680\n",
            "Epoch 109/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5325 - acc: 0.7554 - val_loss: 0.5046 - val_acc: 0.7678\n",
            "Epoch 110/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5320 - acc: 0.7559 - val_loss: 0.5044 - val_acc: 0.7679\n",
            "Epoch 111/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5318 - acc: 0.7559 - val_loss: 0.5041 - val_acc: 0.7678\n",
            "Epoch 112/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5319 - acc: 0.7557 - val_loss: 0.5041 - val_acc: 0.7682\n",
            "Epoch 113/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5317 - acc: 0.7561 - val_loss: 0.5175 - val_acc: 0.7609\n",
            "Epoch 114/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5333 - acc: 0.7551 - val_loss: 0.5039 - val_acc: 0.7682\n",
            "Epoch 115/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5315 - acc: 0.7561 - val_loss: 0.5035 - val_acc: 0.7684\n",
            "Epoch 116/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5313 - acc: 0.7561 - val_loss: 0.5037 - val_acc: 0.7681\n",
            "Epoch 117/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5308 - acc: 0.7564 - val_loss: 0.5032 - val_acc: 0.7685\n",
            "Epoch 118/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5306 - acc: 0.7567 - val_loss: 0.5032 - val_acc: 0.7683\n",
            "Epoch 119/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5309 - acc: 0.7566 - val_loss: 0.5035 - val_acc: 0.7685\n",
            "Epoch 120/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5306 - acc: 0.7567 - val_loss: 0.5031 - val_acc: 0.7689\n",
            "Epoch 121/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5308 - acc: 0.7565 - val_loss: 0.5031 - val_acc: 0.7683\n",
            "Epoch 122/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5302 - acc: 0.7568 - val_loss: 0.5029 - val_acc: 0.7687\n",
            "Epoch 123/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5306 - acc: 0.7564 - val_loss: 0.5027 - val_acc: 0.7686\n",
            "Epoch 124/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5300 - acc: 0.7570 - val_loss: 0.5028 - val_acc: 0.7690\n",
            "Epoch 125/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5299 - acc: 0.7572 - val_loss: 0.5022 - val_acc: 0.7692\n",
            "Epoch 126/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5302 - acc: 0.7570 - val_loss: 0.5023 - val_acc: 0.7692\n",
            "Epoch 127/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5297 - acc: 0.7573 - val_loss: 0.5021 - val_acc: 0.7691\n",
            "Epoch 128/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5296 - acc: 0.7575 - val_loss: 0.5031 - val_acc: 0.7688\n",
            "Epoch 129/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5296 - acc: 0.7574 - val_loss: 0.5021 - val_acc: 0.7695\n",
            "Epoch 130/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5307 - acc: 0.7568 - val_loss: 0.5020 - val_acc: 0.7692\n",
            "Epoch 131/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5292 - acc: 0.7578 - val_loss: 0.5017 - val_acc: 0.7698\n",
            "Epoch 132/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5291 - acc: 0.7577 - val_loss: 0.5014 - val_acc: 0.7698\n",
            "Epoch 133/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5292 - acc: 0.7576 - val_loss: 0.5015 - val_acc: 0.7698\n",
            "Epoch 134/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5291 - acc: 0.7581 - val_loss: 0.5018 - val_acc: 0.7700\n",
            "Epoch 135/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5289 - acc: 0.7582 - val_loss: 0.5013 - val_acc: 0.7698\n",
            "Epoch 136/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5288 - acc: 0.7584 - val_loss: 0.5017 - val_acc: 0.7700\n",
            "Epoch 137/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5287 - acc: 0.7585 - val_loss: 0.5015 - val_acc: 0.7700\n",
            "Epoch 138/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5286 - acc: 0.7585 - val_loss: 0.5010 - val_acc: 0.7703\n",
            "Epoch 139/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5288 - acc: 0.7582 - val_loss: 0.5016 - val_acc: 0.7700\n",
            "Epoch 140/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5284 - acc: 0.7587 - val_loss: 0.5010 - val_acc: 0.7704\n",
            "Epoch 141/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5284 - acc: 0.7586 - val_loss: 0.5013 - val_acc: 0.7707\n",
            "Epoch 142/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5286 - acc: 0.7585 - val_loss: 0.5026 - val_acc: 0.7696\n",
            "Epoch 143/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5282 - acc: 0.7588 - val_loss: 0.5004 - val_acc: 0.7707\n",
            "Epoch 144/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5279 - acc: 0.7588 - val_loss: 0.5005 - val_acc: 0.7704\n",
            "Epoch 145/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5280 - acc: 0.7587 - val_loss: 0.5004 - val_acc: 0.7706\n",
            "Epoch 146/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5279 - acc: 0.7590 - val_loss: 0.5015 - val_acc: 0.7701\n",
            "Epoch 147/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5277 - acc: 0.7590 - val_loss: 0.5017 - val_acc: 0.7702\n",
            "Epoch 148/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5275 - acc: 0.7592 - val_loss: 0.5005 - val_acc: 0.7708\n",
            "Epoch 149/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5274 - acc: 0.7592 - val_loss: 0.5014 - val_acc: 0.7705\n",
            "Epoch 150/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5273 - acc: 0.7594 - val_loss: 0.5003 - val_acc: 0.7709\n",
            "Epoch 151/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5274 - acc: 0.7594 - val_loss: 0.4999 - val_acc: 0.7712\n",
            "Epoch 152/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5272 - acc: 0.7595 - val_loss: 0.4998 - val_acc: 0.7712\n",
            "Epoch 153/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5272 - acc: 0.7595 - val_loss: 0.4997 - val_acc: 0.7714\n",
            "Epoch 154/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5271 - acc: 0.7595 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 155/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5277 - acc: 0.7591 - val_loss: 0.5000 - val_acc: 0.7712\n",
            "Epoch 156/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5273 - acc: 0.7593 - val_loss: 0.4996 - val_acc: 0.7713\n",
            "Epoch 157/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5268 - acc: 0.7599 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 158/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5269 - acc: 0.7596 - val_loss: 0.5003 - val_acc: 0.7709\n",
            "Epoch 159/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5268 - acc: 0.7599 - val_loss: 0.4994 - val_acc: 0.7718\n",
            "Epoch 160/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5267 - acc: 0.7599 - val_loss: 0.4996 - val_acc: 0.7714\n",
            "Epoch 161/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5265 - acc: 0.7602 - val_loss: 0.4991 - val_acc: 0.7716\n",
            "Epoch 162/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5267 - acc: 0.7599 - val_loss: 0.5020 - val_acc: 0.7694\n",
            "Epoch 163/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5266 - acc: 0.7600 - val_loss: 0.4989 - val_acc: 0.7718\n",
            "Epoch 164/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5263 - acc: 0.7604 - val_loss: 0.4993 - val_acc: 0.7717\n",
            "Epoch 165/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5262 - acc: 0.7603 - val_loss: 0.4992 - val_acc: 0.7716\n",
            "Epoch 166/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5264 - acc: 0.7599 - val_loss: 0.4992 - val_acc: 0.7718\n",
            "Epoch 167/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5262 - acc: 0.7602 - val_loss: 0.4990 - val_acc: 0.7718\n",
            "Epoch 168/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5319 - acc: 0.7564 - val_loss: 0.5015 - val_acc: 0.7699\n",
            "Epoch 169/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5278 - acc: 0.7592 - val_loss: 0.4994 - val_acc: 0.7715\n",
            "Epoch 170/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5267 - acc: 0.7601 - val_loss: 0.5001 - val_acc: 0.7714\n",
            "Epoch 171/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5264 - acc: 0.7603 - val_loss: 0.4990 - val_acc: 0.7717\n",
            "Epoch 172/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5261 - acc: 0.7604 - val_loss: 0.4986 - val_acc: 0.7717\n",
            "Epoch 173/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5261 - acc: 0.7604 - val_loss: 0.5007 - val_acc: 0.7712\n",
            "Epoch 174/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5260 - acc: 0.7605 - val_loss: 0.4986 - val_acc: 0.7721\n",
            "Epoch 175/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5260 - acc: 0.7605 - val_loss: 0.4994 - val_acc: 0.7719\n",
            "Epoch 176/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5258 - acc: 0.7605 - val_loss: 0.4989 - val_acc: 0.7723\n",
            "Epoch 177/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5263 - acc: 0.7603 - val_loss: 0.4996 - val_acc: 0.7719\n",
            "Epoch 178/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5260 - acc: 0.7606 - val_loss: 0.4993 - val_acc: 0.7721\n",
            "Epoch 179/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5258 - acc: 0.7608 - val_loss: 0.4983 - val_acc: 0.7722\n",
            "Epoch 180/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5257 - acc: 0.7608 - val_loss: 0.4982 - val_acc: 0.7724\n",
            "Epoch 181/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5256 - acc: 0.7608 - val_loss: 0.4982 - val_acc: 0.7722\n",
            "Epoch 182/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5256 - acc: 0.7609 - val_loss: 0.4987 - val_acc: 0.7723\n",
            "Epoch 183/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5259 - acc: 0.7607 - val_loss: 0.4984 - val_acc: 0.7724\n",
            "Epoch 184/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5255 - acc: 0.7610 - val_loss: 0.4983 - val_acc: 0.7719\n",
            "Epoch 185/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5256 - acc: 0.7609 - val_loss: 0.4981 - val_acc: 0.7724\n",
            "Epoch 186/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5254 - acc: 0.7610 - val_loss: 0.4986 - val_acc: 0.7724\n",
            "Epoch 187/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5253 - acc: 0.7611 - val_loss: 0.4989 - val_acc: 0.7724\n",
            "Epoch 188/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5254 - acc: 0.7612 - val_loss: 0.4979 - val_acc: 0.7725\n",
            "Epoch 189/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5255 - acc: 0.7610 - val_loss: 0.4982 - val_acc: 0.7725\n",
            "Epoch 190/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4985 - val_acc: 0.7724\n",
            "Epoch 191/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4982 - val_acc: 0.7726\n",
            "Epoch 192/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4982 - val_acc: 0.7726\n",
            "Epoch 193/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5253 - acc: 0.7612 - val_loss: 0.4979 - val_acc: 0.7724\n",
            "Epoch 194/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5251 - acc: 0.7613 - val_loss: 0.4978 - val_acc: 0.7726\n",
            "Epoch 195/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5252 - acc: 0.7612 - val_loss: 0.4986 - val_acc: 0.7723\n",
            "Epoch 196/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4986 - val_acc: 0.7722\n",
            "Epoch 197/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5250 - acc: 0.7612 - val_loss: 0.4989 - val_acc: 0.7714\n",
            "Epoch 198/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5251 - acc: 0.7612 - val_loss: 0.4981 - val_acc: 0.7720\n",
            "Epoch 199/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5250 - acc: 0.7614 - val_loss: 0.4982 - val_acc: 0.7722\n",
            "Epoch 200/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5251 - acc: 0.7613 - val_loss: 0.4989 - val_acc: 0.7724\n",
            "Epoch 201/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5248 - acc: 0.7614 - val_loss: 0.4976 - val_acc: 0.7728\n",
            "Epoch 202/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5253 - acc: 0.7612 - val_loss: 0.4976 - val_acc: 0.7729\n",
            "Epoch 203/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5246 - acc: 0.7617 - val_loss: 0.4997 - val_acc: 0.7719\n",
            "Epoch 204/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5249 - acc: 0.7614 - val_loss: 0.4979 - val_acc: 0.7726\n",
            "Epoch 205/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5265 - acc: 0.7606 - val_loss: 0.4978 - val_acc: 0.7727\n",
            "Epoch 206/500\n",
            "4061/4061 [==============================] - 77s 19ms/step - loss: 0.5248 - acc: 0.7615 - val_loss: 0.4973 - val_acc: 0.7724\n",
            "Epoch 207/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5246 - acc: 0.7616 - val_loss: 0.4982 - val_acc: 0.7720\n",
            "Epoch 208/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5247 - acc: 0.7615 - val_loss: 0.4988 - val_acc: 0.7725\n",
            "Epoch 209/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5247 - acc: 0.7615 - val_loss: 0.4975 - val_acc: 0.7724\n",
            "Epoch 210/500\n",
            "4061/4061 [==============================] - 86s 21ms/step - loss: 0.5245 - acc: 0.7617 - val_loss: 0.4982 - val_acc: 0.7726\n",
            "Epoch 211/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5315 - acc: 0.7582 - val_loss: 0.4980 - val_acc: 0.7721\n",
            "Epoch 212/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5248 - acc: 0.7615 - val_loss: 0.4972 - val_acc: 0.7728\n",
            "Epoch 213/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5244 - acc: 0.7618 - val_loss: 0.4975 - val_acc: 0.7727\n",
            "Epoch 214/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5244 - acc: 0.7617 - val_loss: 0.4976 - val_acc: 0.7729\n",
            "Epoch 215/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5250 - acc: 0.7614 - val_loss: 0.4969 - val_acc: 0.7730\n",
            "Epoch 216/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5242 - acc: 0.7617 - val_loss: 0.4973 - val_acc: 0.7728\n",
            "Epoch 217/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7618 - val_loss: 0.4973 - val_acc: 0.7730\n",
            "Epoch 218/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7618 - val_loss: 0.4970 - val_acc: 0.7731\n",
            "Epoch 219/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5242 - acc: 0.7618 - val_loss: 0.4970 - val_acc: 0.7730\n",
            "Epoch 220/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5241 - acc: 0.7618 - val_loss: 0.4970 - val_acc: 0.7731\n",
            "Epoch 221/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7618 - val_loss: 0.5003 - val_acc: 0.7714\n",
            "Epoch 222/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4973 - val_acc: 0.7733\n",
            "Epoch 223/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4969 - val_acc: 0.7731\n",
            "Epoch 224/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5245 - acc: 0.7617 - val_loss: 0.4983 - val_acc: 0.7726\n",
            "Epoch 225/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7620 - val_loss: 0.4966 - val_acc: 0.7732\n",
            "Epoch 226/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5256 - acc: 0.7612 - val_loss: 0.5022 - val_acc: 0.7717\n",
            "Epoch 227/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5249 - acc: 0.7615 - val_loss: 0.4966 - val_acc: 0.7733\n",
            "Epoch 228/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4973 - val_acc: 0.7732\n",
            "Epoch 229/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5240 - acc: 0.7621 - val_loss: 0.4968 - val_acc: 0.7732\n",
            "Epoch 230/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5240 - acc: 0.7621 - val_loss: 0.4966 - val_acc: 0.7733\n",
            "Epoch 231/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5239 - acc: 0.7621 - val_loss: 0.4969 - val_acc: 0.7731\n",
            "Epoch 232/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5239 - acc: 0.7620 - val_loss: 0.4965 - val_acc: 0.7732\n",
            "Epoch 233/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4972 - val_acc: 0.7731\n",
            "Epoch 234/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7623 - val_loss: 0.4967 - val_acc: 0.7735\n",
            "Epoch 235/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5239 - acc: 0.7622 - val_loss: 0.4966 - val_acc: 0.7734\n",
            "Epoch 236/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7623 - val_loss: 0.4970 - val_acc: 0.7732\n",
            "Epoch 237/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5240 - acc: 0.7619 - val_loss: 0.4967 - val_acc: 0.7733\n",
            "Epoch 238/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5237 - acc: 0.7622 - val_loss: 0.4961 - val_acc: 0.7736\n",
            "Epoch 239/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5237 - acc: 0.7624 - val_loss: 0.4971 - val_acc: 0.7734\n",
            "Epoch 240/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5239 - acc: 0.7623 - val_loss: 0.4968 - val_acc: 0.7733\n",
            "Epoch 241/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5235 - acc: 0.7625 - val_loss: 0.4964 - val_acc: 0.7735\n",
            "Epoch 242/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7623 - val_loss: 0.4968 - val_acc: 0.7736\n",
            "Epoch 243/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5236 - acc: 0.7623 - val_loss: 0.4970 - val_acc: 0.7731\n",
            "Epoch 244/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5241 - acc: 0.7620 - val_loss: 0.4968 - val_acc: 0.7733\n",
            "Epoch 245/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5239 - acc: 0.7622 - val_loss: 0.4961 - val_acc: 0.7737\n",
            "Epoch 246/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5235 - acc: 0.7626 - val_loss: 0.4971 - val_acc: 0.7736\n",
            "Epoch 247/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5235 - acc: 0.7625 - val_loss: 0.4980 - val_acc: 0.7727\n",
            "Epoch 248/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5235 - acc: 0.7625 - val_loss: 0.4962 - val_acc: 0.7733\n",
            "Epoch 249/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5234 - acc: 0.7624 - val_loss: 0.4961 - val_acc: 0.7739\n",
            "Epoch 250/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5235 - acc: 0.7624 - val_loss: 0.4963 - val_acc: 0.7734\n",
            "Epoch 251/500\n",
            "4061/4061 [==============================] - 77s 19ms/step - loss: 0.5236 - acc: 0.7624 - val_loss: 0.4960 - val_acc: 0.7737\n",
            "Epoch 252/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5251 - acc: 0.7615 - val_loss: 0.4994 - val_acc: 0.7729\n",
            "Epoch 253/500\n",
            "4061/4061 [==============================] - 77s 19ms/step - loss: 0.5241 - acc: 0.7621 - val_loss: 0.4963 - val_acc: 0.7734\n",
            "Epoch 254/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5231 - acc: 0.7626 - val_loss: 0.4957 - val_acc: 0.7737\n",
            "Epoch 255/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5232 - acc: 0.7627 - val_loss: 0.4957 - val_acc: 0.7738\n",
            "Epoch 256/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5232 - acc: 0.7627 - val_loss: 0.4958 - val_acc: 0.7739\n",
            "Epoch 257/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5964 - acc: 0.7412 - val_loss: 0.5569 - val_acc: 0.7484\n",
            "Epoch 258/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5717 - acc: 0.7388 - val_loss: 0.5283 - val_acc: 0.7567\n",
            "Epoch 259/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5520 - acc: 0.7451 - val_loss: 0.5143 - val_acc: 0.7628\n",
            "Epoch 260/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5397 - acc: 0.7516 - val_loss: 0.5049 - val_acc: 0.7671\n",
            "Epoch 261/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5328 - acc: 0.7563 - val_loss: 0.5012 - val_acc: 0.7705\n",
            "Epoch 262/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5298 - acc: 0.7587 - val_loss: 0.4995 - val_acc: 0.7718\n",
            "Epoch 263/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5280 - acc: 0.7598 - val_loss: 0.4983 - val_acc: 0.7722\n",
            "Epoch 264/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5269 - acc: 0.7603 - val_loss: 0.4976 - val_acc: 0.7727\n",
            "Epoch 265/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5260 - acc: 0.7606 - val_loss: 0.4972 - val_acc: 0.7726\n",
            "Epoch 266/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5253 - acc: 0.7608 - val_loss: 0.4970 - val_acc: 0.7728\n",
            "Epoch 267/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5247 - acc: 0.7611 - val_loss: 0.4970 - val_acc: 0.7729\n",
            "Epoch 268/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5242 - acc: 0.7615 - val_loss: 0.4964 - val_acc: 0.7732\n",
            "Epoch 269/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7618 - val_loss: 0.4962 - val_acc: 0.7734\n",
            "Epoch 270/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5234 - acc: 0.7621 - val_loss: 0.4961 - val_acc: 0.7735\n",
            "Epoch 271/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5232 - acc: 0.7622 - val_loss: 0.4960 - val_acc: 0.7735\n",
            "Epoch 272/500\n",
            "4061/4061 [==============================] - 83s 20ms/step - loss: 0.5231 - acc: 0.7623 - val_loss: 0.4960 - val_acc: 0.7736\n",
            "Epoch 273/500\n",
            "4061/4061 [==============================] - 83s 21ms/step - loss: 0.5230 - acc: 0.7623 - val_loss: 0.4963 - val_acc: 0.7734\n",
            "Epoch 274/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5230 - acc: 0.7623 - val_loss: 0.4958 - val_acc: 0.7738\n",
            "Epoch 275/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4961 - val_acc: 0.7735\n",
            "Epoch 276/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4957 - val_acc: 0.7738\n",
            "Epoch 277/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4959 - val_acc: 0.7737\n",
            "Epoch 278/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4962 - val_acc: 0.7735\n",
            "Epoch 279/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5229 - acc: 0.7628 - val_loss: 0.4959 - val_acc: 0.7738\n",
            "Epoch 280/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5228 - acc: 0.7628 - val_loss: 0.4956 - val_acc: 0.7738\n",
            "Epoch 281/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5228 - acc: 0.7628 - val_loss: 0.4956 - val_acc: 0.7739\n",
            "Epoch 282/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4962 - val_acc: 0.7734\n",
            "Epoch 283/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5228 - acc: 0.7630 - val_loss: 0.4961 - val_acc: 0.7736\n",
            "Epoch 284/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5228 - acc: 0.7630 - val_loss: 0.4958 - val_acc: 0.7737\n",
            "Epoch 285/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4956 - val_acc: 0.7739\n",
            "Epoch 286/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4965 - val_acc: 0.7736\n",
            "Epoch 287/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5229 - acc: 0.7629 - val_loss: 0.4957 - val_acc: 0.7739\n",
            "Epoch 288/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4963 - val_acc: 0.7736\n",
            "Epoch 289/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5228 - acc: 0.7629 - val_loss: 0.4962 - val_acc: 0.7736\n",
            "Epoch 290/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5305 - acc: 0.7590 - val_loss: 0.4969 - val_acc: 0.7736\n",
            "Epoch 291/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5233 - acc: 0.7627 - val_loss: 0.4961 - val_acc: 0.7736\n",
            "Epoch 292/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5229 - acc: 0.7628 - val_loss: 0.4956 - val_acc: 0.7737\n",
            "Epoch 293/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5228 - acc: 0.7628 - val_loss: 0.4965 - val_acc: 0.7736\n",
            "Epoch 294/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4960 - val_acc: 0.7737\n",
            "Epoch 295/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4955 - val_acc: 0.7738\n",
            "Epoch 296/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5226 - acc: 0.7630 - val_loss: 0.4955 - val_acc: 0.7738\n",
            "Epoch 297/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5226 - acc: 0.7630 - val_loss: 0.4959 - val_acc: 0.7737\n",
            "Epoch 298/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5230 - acc: 0.7629 - val_loss: 0.4960 - val_acc: 0.7748\n",
            "Epoch 299/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5231 - acc: 0.7629 - val_loss: 0.4954 - val_acc: 0.7738\n",
            "Epoch 300/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4972 - val_acc: 0.7731\n",
            "Epoch 301/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5228 - acc: 0.7629 - val_loss: 0.4961 - val_acc: 0.7739\n",
            "Epoch 302/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5229 - acc: 0.7628 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 303/500\n",
            "4061/4061 [==============================] - 83s 20ms/step - loss: 0.5229 - acc: 0.7627 - val_loss: 0.4955 - val_acc: 0.7740\n",
            "Epoch 304/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 305/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4958 - val_acc: 0.7739\n",
            "Epoch 306/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5226 - acc: 0.7629 - val_loss: 0.4963 - val_acc: 0.7737\n",
            "Epoch 307/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5226 - acc: 0.7631 - val_loss: 0.4953 - val_acc: 0.7741\n",
            "Epoch 308/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4955 - val_acc: 0.7739\n",
            "Epoch 309/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5226 - acc: 0.7631 - val_loss: 0.4955 - val_acc: 0.7741\n",
            "Epoch 310/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 311/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5228 - acc: 0.7632 - val_loss: 0.4955 - val_acc: 0.7743\n",
            "Epoch 312/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5225 - acc: 0.7632 - val_loss: 0.4952 - val_acc: 0.7743\n",
            "Epoch 313/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4958 - val_acc: 0.7740\n",
            "Epoch 314/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4958 - val_acc: 0.7740\n",
            "Epoch 315/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5226 - acc: 0.7631 - val_loss: 0.4957 - val_acc: 0.7741\n",
            "Epoch 316/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4954 - val_acc: 0.7743\n",
            "Epoch 317/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5225 - acc: 0.7633 - val_loss: 0.4966 - val_acc: 0.7737\n",
            "Epoch 318/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4965 - val_acc: 0.7736\n",
            "Epoch 319/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5224 - acc: 0.7634 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 320/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5225 - acc: 0.7632 - val_loss: 0.4984 - val_acc: 0.7726\n",
            "Epoch 321/500\n",
            "4032/4061 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7635"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-c84f1e47454c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0mfinish_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE5tuzfRS9SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXOucdnVW4mF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "15d9f3b7-03cc-4baf-83c3-c6eba286d356"
      },
      "source": [
        "import random as rnd\n",
        "row=rnd.randint(1,800)\n",
        "clm=rnd.randint(1,150)\n",
        "\n",
        "print('row: ',row)\n",
        "print('column: ',clm)\n",
        "\n",
        "big=max(y_pred[row][clm])\n",
        "arr=[]\n",
        "for item in y_pred[row][clm]:\n",
        "  if(item==big):\n",
        "    arr.append(1)\n",
        "  else:\n",
        "    arr.append(0)\n",
        "    \n",
        "print('actual: ',y_test[row][clm])    \n",
        "print('predicted: ',arr)\n",
        "\n",
        "plt.plot(y_test[row][clm],color='royalblue',label='real')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(arr,color='orange',label='predicted')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "row:  108\n",
            "column:  90\n",
            "actual:  [1 0 0 0 0]\n",
            "predicted:  [1, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIxpHIXEYmh/wQQpxdPVwlH8wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFMEJw76FkFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('my_model_76.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}